---
title: "Machine learning using RNAseq data"
author: "Alsu"
date: "February-March 2024"
output: github_document
---

# Data preparation

```{r tidy-data}
library(janitor)
library(tidyverse)

setwd("/cloud/project/data/")  

normalized.counts.ibd <- read.table(file="Normalized_Count.csv",
                             sep="",
                             header=T,
                             fill=T,
                             check.names=F)

# Transpose data frame
data <- t(normalized.counts.ibd)

# Move the gene names as headers, add column for status
data <- data %>%
  row_to_names(row_number = 1) %>%
  as.data.frame() # %>%

```


```{r splitting-data}
#install.packages('rsample')

library(rsample)

# Fix random numbers by setting the seed 
# Enables analysis to be reproducible when random numbers are used 
set.seed(123)
# Put 80% of the data into the training set 
data_split <- initial_split(data, prop = 0.70)
# Create data frames for the two sets:
train_data <- training(data_split)
test_data  <- testing(data_split)
```

# Logistic regression after PCA and decision tree
### Dimensionality reduction (PCA)

```{r PCA-for-train-data}
#convert character dataframe to numeric
train_data <- as.data.frame(sapply( train_data , as.numeric))

# Identify constant or zero columns
train_data_transposed_cons <- sapply(train_data, function(x) is.atomic(x) && length(unique(x)) == 1)

# Remove constant or zero columns
train_data_transposed_no <- train_data[, !train_data_transposed_cons]

# Run PCA
pca_train_result <- prcomp(train_data_transposed_no, scale. = TRUE)

```

```{r visualize-pca}
# Plot PCA results
plot(pca_train_result$x[,1], pca_train_result$x[,2], 
     xlab = "PC1", ylab = "PC2", 
     main = "PCA of Normalized Counts for the training data")

# 3D scatterplot

#install.packages("scatterplot3d") #install as needed
library(scatterplot3d)

scatterplot3d(pca_train_result$x[,1], pca_train_result$x[,2], pca_train_result$x[,3], 
              xlab = "PC1", ylab = "PC2", zlab = "PC3", 
              main = "3D PCA Plot")

#Screeplot
screeplot(pca_train_result, type = "lines")

#heatmap
train_matrix <- as.matrix(train_data[, -1])   
train_matrix <- matrix(as.numeric(unlist(train_matrix)),nrow=nrow(train_matrix))

#heatmap(train_matrix, scale = "row", add.expr = TRUE) #PositCloud does not have enought computing power for this one
```


### Logistic regression

```{r prepare-data-logistic-reg}
# Select first few principal components (based on screeplot)
num_components <- 5
selected_pcs <- pca_train_result$x[, 1:num_components]

#Add binary variable (parkinson's vs control). Condition is written for the seed(123)
condition <- c(rep("Control", 1), rep("Parkinsons", 2), rep("Control", 4), rep("Parkinsons", 2))

# Assuming 'condition' is the binary outcome variable
data_train_selected_pcs <- cbind(selected_pcs, condition)

```

```{r fit-the-model}
library(broom)
library(parsnip)

data_train_selected_pcs <- as.data.frame(data_train_selected_pcs)

data_train_selected_pcs <- transform(data_train_selected_pcs, 
            PC1 = as.numeric(PC1), 
            PC2 = as.numeric(PC2),
            PC3 = as.numeric(PC3), 
            PC4 = as.numeric(PC4),
            PC5 = as.numeric(PC5)
          )

#Fitting the logistic regression model
model_fit <- logistic_reg() %>%
  set_engine("glm") %>%
  fit(as.factor(condition) ~ ., data = data_train_selected_pcs, family = "binomial")

tidy(model_fit)

#Assessing the goodness of fit of the logistic regression model
summary(model_fit)

```

```{r extract-variables}

# Making gene list
gene_names <- as.matrix.data.frame(train_data_transposed_no[0,], col_names = FALSE)

# Assuming 'pca_train_result' contains the PCA result object obtained from prcomp
# Assuming 'gene_names' contains the names of the genes in the same order as the columns in my count data matrix

# Get loadings of original variables on principal components
loadings <- pca_train_result$rotation

# Define function to get top contributing genes for each principal component
get_top_genes <- function(component_index, num_genes = 10) {
  # Sort genes by absolute loading value for the specified principal component
  sorted_genes <- order(abs(loadings[, component_index]), decreasing = TRUE)
  # Get top contributing gene names
  top_genes <- gene_names[sorted_genes[1:num_genes]]
  # Get corresponding loadings
  top_loadings <- loadings[sorted_genes[1:num_genes], component_index]
  # Create data frame with gene names and loadings
  top_genes_df <- data.frame(Gene = top_genes, Loading = top_loadings)
  return(top_genes_df)
}

# Example: Get top contributing genes for the first principal component
top_genes_pc1 <- get_top_genes(1, num_genes = 10)
print(top_genes_pc1)

```

### Decision tree

```{r build-model-for-dec-tree}
library(tidymodels)
library(themis)
library(glmnet)
library(parsnip)
library(recipes)

# Create recipe
park_recipe <- recipe(condition ~ ., data = data_train_selected_pcs) %>%
                step_normalize(all_numeric_predictors())

## Build a logistic regression model

glm_park <- logistic_reg() %>%
  set_engine("glm")

## Start a workflow (recipe only)
park_wf <- workflow() %>%
    add_recipe(park_recipe)

## Add the model and fit the workflow
park_glm <- park_wf %>%
    add_model(glm_park) %>%
    fit(data = data_train_selected_pcs)

# Print the fitted model
park_glm
```


```{r train decision-tree}
library(tidymodels)
library(themis)

park_pca_recipe <- recipe(condition ~ ., data = data_train_selected_pcs) %>%
                step_normalize(all_numeric_predictors()) 

## Build a decision tree model
tree_pca_park <- decision_tree() %>%         
    set_engine("rpart") %>%      
    set_mode("classification") 

## Start a workflow (recipe only)
park_wf <- workflow() %>%
    add_recipe(park_pca_recipe)

## Add the model and fit the workflow
park_tree <- park_wf %>%
    add_model(tree_pca_park) %>%
    fit(data = data_train_selected_pcs)

# Print the fitted model
park_tree
```


```{r confusion-matrix}
data_train_selected_pcs$condition <- as.factor(data_train_selected_pcs$condition)

results <- data_train_selected_pcs %>%
    bind_cols(predict(park_glm, data_train_selected_pcs) %>%
                  rename(.pred_glm = .pred_class))

# Confusion matrix for logistic regression model
results %>%
    conf_mat(truth = condition, estimate = .pred_glm)

results <- data_train_selected_pcs %>%
    bind_cols(predict(park_glm, data_train_selected_pcs) %>%
                  rename(.pred_tree = .pred_class))

# Confusion matrix for decision tree model
results %>%
    conf_mat(truth = condition, estimate = .pred_tree)
```

```{r accuracy}
#convert character dataframe to numeric
test_data <- as.data.frame(sapply(test_data , as.numeric))

# Identify constant or zero columns
test_data_transposed_cons <- sapply(test_data, function(x) is.atomic(x) && length(unique(x)) == 1)

# Remove constant or zero columns
test_data_transposed_no <- test_data[, !test_data_transposed_cons]

# Run PCA
pca_test_result <- prcomp(test_data_transposed_no, scale. = TRUE)

#Select first 6 PCAs
num_components <- 5
test_data_selected_pcs <- as.data.frame(pca_test_result$x[, 1:num_components])

condition_test <- c(rep("Control", 1), rep("Parkinsons", 4))

# Assuming 'condition' is the binary outcome variable
test_data_selected_pcs <- cbind(test_data_selected_pcs, condition_test)
test_data_selected_pcs$condition <- as.factor(test_data_selected_pcs$condition)

# Accuracy
library(tidymodels)

results_test <- test_data_selected_pcs %>%
    bind_cols(predict(park_glm, test_data_selected_pcs) %>%
                  rename(.pred_glm = .pred_class)) %>%
    bind_cols(predict(park_glm, test_data_selected_pcs) %>%
                  rename(.pred_tree = .pred_class))

# Calculate accuracy
accuracy(results_test, truth = condition, estimate = .pred_glm)
accuracy(results_test, truth = condition, estimate = .pred_tree)

# Calculate positive predict value
ppv(results_test, truth = condition, estimate = .pred_glm)
ppv(results_test, truth = condition, estimate = .pred_tree)
```


# LASSO

```{r create-condition-for-train-data}

train_data <- cbind(condition, train_data) #make sure only runs once
```


```{r logistic-reg}
library(tidymodels)
library(themis)
library(glmnet)
library(parsnip)
library(recipes)

mini_train <- train_data[, c(1:100)]

# Create recipe
park_recipe2 <- recipe(condition ~ ., data = mini_train) %>%
                step_normalize(all_predictors())

## Build a logistic regression model

glm_park2 <- logistic_reg(penalty = 0.5, mixture = 1) %>%
    set_engine("glm")

## Start a workflow (recipe only)
park_wf2 <- workflow() %>%
    add_recipe(park_recipe2)

## Add the model and fit the workflow
park_glm2 <- park_wf2 %>%
    add_model(glm_park2) %>%
    fit(data = mini_train)

# Print the fitted model
park_glm2

# Oh no :( My dataset doesn't have enought degrees of freedom.
```