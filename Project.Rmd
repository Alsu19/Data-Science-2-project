---
title: "Machine learning using RNAseq data"
author: "Alsu"
date: "February-March 2024"
output: github_document
---

# Data preparation

```{r tidy-data}
library(janitor)
library(tidyverse)

setwd("/cloud/project/data/")  

normalized.counts.ibd <- read.table(file="Normalized_Count.csv",
                             sep="",
                             header=T,
                             fill=T,
                             check.names=F)

# Transpose data frame
data <- t(normalized.counts.ibd)

# Move the gene names as headers, add column for status
data <- data %>%
  row_to_names(row_number = 1) %>%
  as.data.frame() # %>%

```


```{r splitting-data}
#install.packages('rsample')

library(rsample)

# Fix random numbers by setting the seed 
# Enables analysis to be reproducible when random numbers are used 
set.seed(123)
# Put 80% of the data into the training set 
data_split <- initial_split(data, prop = 0.70)
# Create data frames for the two sets:
train_data <- training(data_split)
test_data  <- testing(data_split)
```

# Logistic regression after PCA
### Dimensionality reduction (PCA)

```{r PCA-for-train-data}
#convert character dataframe to numeric
train_data <- as.data.frame(sapply( train_data , as.numeric))

# Identify constant or zero columns
train_data_transposed_cons <- sapply(train_data, function(x) is.atomic(x) && length(unique(x)) == 1)

# Remove constant or zero columns
train_data_transposed_no <- train_data[, !train_data_transposed_cons]

# Run PCA
pca_train_result <- prcomp(train_data_transposed_no, scale. = TRUE)

```

```{r visualize-pca}
# Plot PCA results
plot(pca_train_result$x[,1], pca_train_result$x[,2], 
     xlab = "PC1", ylab = "PC2", 
     main = "PCA of Normalized Counts for the training data")

# 3D scatterplot

#install.packages("scatterplot3d") #install as needed
library(scatterplot3d)

scatterplot3d(pca_train_result$x[,1], pca_train_result$x[,2], pca_train_result$x[,3], 
              xlab = "PC1", ylab = "PC2", zlab = "PC3", 
              main = "3D PCA Plot")

#Screeplot
screeplot(pca_train_result, type = "lines")

#heatmap
train_matrix <- as.matrix(train_data[, -1])   
train_matrix <- matrix(as.numeric(unlist(train_matrix)),nrow=nrow(train_matrix))

#heatmap(train_matrix, scale = "row", add.expr = TRUE) #PositCloud does not have enought computing power for this one
```


### Logistic regression

```{r prepare-data-logistic-reg}
# Select first few principal components (based on screeplot)
num_components <- 6
selected_pcs <- pca_train_result$x[, 1:num_components]

#Add binary variable (parkinson's vs control). Condition is written for the seed(123)
condition <- c(rep("Control", 1), rep("Parkinsons", 2), rep("Control", 4), rep("Parkinsons", 2))

# Assuming 'condition' is the binary outcome variable
data_train_selected_pcs <- cbind(selected_pcs, condition)

```

```{r fit-the-model}
library(broom)
library(parsnip)

data_train_selected_pcs <- as.data.frame(data_train_selected_pcs)

data_train_selected_pcs <- transform(data_train_selected_pcs, 
            PC1 = as.numeric(PC1), 
            PC2 = as.numeric(PC2),
            PC3 = as.numeric(PC3), 
            PC4 = as.numeric(PC4),
            PC5 = as.numeric(PC5), 
            PC6 = as.numeric(PC6))

#Fitting the logistic regression model
model_fit <- logistic_reg() %>%
  set_engine("glm") %>%
  fit(as.factor(condition) ~ ., data = data_train_selected_pcs, family = "binomial")

tidy(model_fit)

#Assessing the goodness of fit of the logistic regression model
summary(model_fit)

```

```{r}

```


# Logistic regression without PCA

```{r build-model}
#install.packages("glmnet", "parsnip") #install as needed

library(glmnet)
library(parsnip)

lr_mod <- 
  logistic_reg(penalty = tune(), mixture = 1) %>% 
  set_engine("glmnet")
```


```{r create-recipe}

```

```{r create-workflow}

```

```{r grid-for-tuning}

```

